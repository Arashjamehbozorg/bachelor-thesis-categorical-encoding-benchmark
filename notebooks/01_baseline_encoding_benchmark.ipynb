{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01 - Baseline Encoding Benchmark Code (Starting Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is created for implementing the baseline categorical encoding benchmark for regression datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First step to setup the environment is to import the necessary tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder as SklearnOrdinalEncoder\n",
    "\n",
    "\n",
    "# category encoders import\n",
    "import category_encoders as ce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to load and prepare the data using try block, so we could find out the error, if there is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Loading and preparing ames dataset\n",
    "    try:\n",
    "        train_data = pd.read_csv(\"../data/raw/ames-housing-dataset/AmesHousing.csv\", index_col=0)\n",
    "\n",
    "        # preprocessing steps\n",
    "        # filling the categorical features missing values\n",
    "        cat_cols_before_exclusion = train_data.select_dtypes(include=\"object\").columns.tolist()\n",
    "        train_data[cat_cols_before_exclusion] = train_data[cat_cols_before_exclusion].fillna(\"missing\")\n",
    "\n",
    "        # filling the numerical features missing values\n",
    "        num_cols_before_exclusion = train_data.select_dtypes(include=\"number\").columns.to_list()\n",
    "        train_data[num_cols_before_exclusion] = train_data[num_cols_before_exclusion].fillna(-1)  \n",
    "\n",
    "        # excluding the useless columns from train data\n",
    "        columns_to_exclude = [\"Order\", \"PID\", \"SalePrice\"]\n",
    "        selected_columns = [col for col in train_data.columns if col not in columns_to_exclude]\n",
    "\n",
    "        # Defining X and y (input and target value)\n",
    "        X = train_data[selected_columns]\n",
    "        y = train_data[\"SalePrice\"]\n",
    "\n",
    "        # column lists after exclusion\n",
    "        cat_columns = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "        num_columns = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "        # return the train data\n",
    "        return X, y, cat_columns, num_columns\n",
    "        \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Dataset was not found!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the variables\n",
    "X, y, cat_columns, num_columns = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define a baseline benchmark class, that we could reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingBenchmark:\n",
    "    \"\"\"\n",
    "    Starting baseline to test core encoder-algorithm interactions.\n",
    "    \n",
    "    Tests 3 encoders named OneHotEncoder, TargetEncoder, OrdinalEncoder with 2 representative algorithms named Ridge and RandomForest to validate the fundamental hypothesis that encoding choice affects performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining the init\n",
    "    def __init__(self, random_state = 42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # encoders\n",
    "        self.encoders = {\n",
    "            \"OneHotEncoder\" : OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"),\n",
    "            \"TargetEncoder\" : ce.TargetEncoder(smoothing=1),\n",
    "            \"OrdinalEncoder\" : SklearnOrdinalEncoder(handle_unknown = \"use_encoded_value\", unknown_value = -1)\n",
    "        }\n",
    "\n",
    "        # machine learning algorithms\n",
    "        self.algorithms = {\n",
    "            \"Ridge\" : Ridge(random_state=self.random_state),\n",
    "            \"RandomForest\" : RandomForestRegressor(n_estimators=50, random_state=self.random_state)\n",
    "        }\n",
    "\n",
    "        # creating pipeline\n",
    "    def create_pipeline(self, encoder_name, algorithm_name, cat_columns, num_columns):\n",
    "            \"\"\"Create preprocessing and algorithming pipeline.\"\"\"\n",
    "            encoder = self.encoders[encoder_name]\n",
    "            algorithm = self.algorithms[algorithm_name]\n",
    "\n",
    "            #preprocessor\n",
    "            preprocessor = ColumnTransformer([\n",
    "                (\"categorical\", encoder, cat_columns),\n",
    "                (\"numerical\", StandardScaler(), num_columns)\n",
    "            ])\n",
    "\n",
    "            #pipeline implementation\n",
    "            pipeline = Pipeline([\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"algorithm\", algorithm)\n",
    "            ])\n",
    "            return pipeline\n",
    "        \n",
    "    def evaluate_combination(self, X, y, encoder_name, algorithm_name, cat_columns, num_columns):\n",
    "            \"\"\"Evaluate one single combination of encoder and algorithm\"\"\"\n",
    "\n",
    "            try:\n",
    "                # create an object of the class Pipeline\n",
    "                new_pipeline = self.create_pipeline(encoder_name, algorithm_name, cat_columns, num_columns)\n",
    "\n",
    "                # define 5-fold cross validation CV\n",
    "                cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=self.random_state)\n",
    "\n",
    "                # Evaluation metrics\n",
    "                rmse = -cross_val_score(new_pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\n",
    "                rmse = np.sqrt(rmse)\n",
    "\n",
    "                # results\n",
    "                result = {\n",
    "                'encoder': encoder_name,\n",
    "                'algorithm': algorithm_name,\n",
    "                'rmse_mean': rmse.mean(),\n",
    "                'rmse_std': rmse.std()\n",
    "                }\n",
    "            \n",
    "                return result\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"There is an error with the combination of algorithm {algorithm_name} and the encoder {encoder_name}\")\n",
    "                return None\n",
    "        \n",
    "        # run the baseline test\n",
    "    def run_baseline_test(self, X, y, cat_columns, num_columns):        \n",
    "        \n",
    "            results = []\n",
    "        \n",
    "            for encoder_name in self.encoders.keys():\n",
    "                for algorithm_name in self.algorithms.keys():\n",
    "                    print(f\"Testing: {encoder_name} + {algorithm_name}\")\n",
    "                \n",
    "                    result = self.evaluate_combination(\n",
    "                        X, y, encoder_name, algorithm_name, cat_columns, num_columns\n",
    "                    )\n",
    "                \n",
    "                    if result:\n",
    "                        results.append(result)\n",
    "                        print(f\"  RMSE: {result['rmse_mean']:,.0f} (±{result['rmse_std']:,.0f})\")\n",
    "\n",
    "        \n",
    "            return pd.DataFrame(results)\n",
    " \n",
    "        # Debug: Check what's in results\n",
    "            \n",
    "  \n",
    "    # analyse the results (RMSE)\n",
    "    def analyse_results(self, results_df):\n",
    "        \"\"\"Analyse  results to validate core hypothesis.\"\"\"\n",
    "        \n",
    "        # Show all results\n",
    "        print(\"\\nAll Results:\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['encoder']:10} + {row['algorithm']:12} = {row['rmse_mean']:>8,.0f}\")\n",
    "        \n",
    "        # Test core hypothesis, Do encoders perform differently?\n",
    "        print(f\"\\nEncoder Performance Ranges:\")\n",
    "        encoder_stats = results_df.groupby('encoder')['rmse_mean'].agg(['mean', 'min', 'max'])\n",
    "        print(encoder_stats)\n",
    "        \n",
    "        # Test algorithm interaction, Do algorithms prefer different encoders?\n",
    "        print(f\"\\nBest Encoder by Algorithm:\")\n",
    "        for algo in results_df['algorithm'].unique():\n",
    "            algo_results = results_df[results_df['algorithm'] == algo]\n",
    "            best_encoder = algo_results.loc[algo_results['rmse_mean'].idxmin(), 'encoder']\n",
    "            best_rmse = algo_results['rmse_mean'].min()\n",
    "            print(f\"{algo:12}: {best_encoder} (RMSE: {best_rmse:,.0f})\")\n",
    "        \n",
    "        # Validation check\n",
    "        rmse_range = results_df['rmse_mean'].max() - results_df['rmse_mean'].min()\n",
    "        print(f\"\\nValidation: RMSE range = {rmse_range:,.0f}\")\n",
    "        \n",
    "        if rmse_range > 1000: \n",
    "            print(\"Baseline validates hypothesis: Encoding choice matters\")\n",
    "        else:\n",
    "            print(\"Warning: Small differences detected. Check implementation.\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define a if condition to run the baseline test using the class defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: OneHotEncoder + Ridge\n",
      "  RMSE: 28,655 (±3,869)\n",
      "Testing: OneHotEncoder + RandomForest\n",
      "  RMSE: 25,977 (±1,209)\n",
      "Testing: TargetEncoder + Ridge\n",
      "  RMSE: 29,527 (±3,161)\n",
      "Testing: TargetEncoder + RandomForest\n",
      "  RMSE: 25,758 (±1,277)\n",
      "Testing: OrdinalEncoder + Ridge\n",
      "  RMSE: 31,450 (±3,189)\n",
      "Testing: OrdinalEncoder + RandomForest\n",
      "  RMSE: 25,762 (±1,276)\n",
      "\n",
      "All Results:\n",
      "OneHotEncoder + Ridge        =   28,655\n",
      "OneHotEncoder + RandomForest =   25,977\n",
      "TargetEncoder + Ridge        =   29,527\n",
      "TargetEncoder + RandomForest =   25,758\n",
      "OrdinalEncoder + Ridge        =   31,450\n",
      "OrdinalEncoder + RandomForest =   25,762\n",
      "\n",
      "Encoder Performance Ranges:\n",
      "                        mean           min           max\n",
      "encoder                                                 \n",
      "OneHotEncoder   27315.723667  25976.849523  28654.597812\n",
      "OrdinalEncoder  28605.775348  25761.947713  31449.602982\n",
      "TargetEncoder   27642.167912  25757.684046  29526.651779\n",
      "\n",
      "Best Encoder by Algorithm:\n",
      "Ridge       : OneHotEncoder (RMSE: 28,655)\n",
      "RandomForest: TargetEncoder (RMSE: 25,758)\n",
      "\n",
      "Validation: RMSE range = 5,692\n",
      "Baseline validates hypothesis: Encoding choice matters\n",
      "\n",
      "Results saved to baseline_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ## Run Baseline Test\n",
    "\n",
    "if X is not None:\n",
    "    # Initialize and run baseline\n",
    "    baseline = EncodingBenchmark(random_state=42)\n",
    "    \n",
    "    # Run the test\n",
    "    results = baseline.run_baseline_test(X, y, cat_columns, num_columns)\n",
    "    \n",
    "    # Analyze results\n",
    "    analysis = baseline.analyse_results(results)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv('../results/baseline_test_results.csv', index=False)\n",
    "    print(f\"\\nResults saved to baseline_test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
